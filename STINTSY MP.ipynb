{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group: I-Need-More-Boolets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Members: <br>\n",
    "Go, Ryan Jefferson <br>\n",
    "Ejercito, Joshua Carl <br>\n",
    "Moraña, Anton Louis <br>\n",
    "Nieva, Samuel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section: S11\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the problem/task and dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This major course output tasked our group to select a real-word dataset from the selection given. This group has selected the stars dataset, which is a dataset containing spectral characteristics derived from pictures taken by the SDSS (Sloan Digital Sky Survey). This dataset's main use is spectral classification, i.e., classifying whether a celestial body is a star, a quasar or an entire galaxy.\n",
    "\n",
    "Using the dataset, the group is to perform the following:\n",
    "\n",
    "- Describe the dataset\n",
    "- Perform Data Pre-processing and Cleaning\n",
    "- Perform Exploratory Data Analysis\n",
    "- Select 3 Machine Learning Models\n",
    "- Perform model training\n",
    "- Perform hyperparameter tuning\n",
    "- Extract insights from the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated earlier, the Stars datasets contains spectral characteristics from celestial bodies that can be used to classify it as either a quasar, star, or galaxy. This dataset is made up of 100,000 obersavations of space taken by the SDSS (Sloan Digital Sky Survey) with each observations having 18 features. The SDSS takes these observations through the use of a 2.5m wide located at Apache Point Observatory in New Mexico. The telescope is described in detail in a paper by Gunn et al. (2006), and it is through this powerful telescope that a robust astronomical survey is created, from which datasets such as this are derived.\n",
    "\n",
    "The following are the description of their features:\n",
    "\n",
    "1. obj_ID =  the unique value that identifies the object in the image catalog used by th CAS (Chinese Academy of Sciences).\n",
    "2. alpha = it contains the right ascension angle, it is measured from the start of a point which is called vernal equinox and go eastward. It is expressed in hours, minutes, and seconds. Vernal equinox is when the sun is exactly above the equator and day and night are of equal day. It is similar to the longitude in space.\n",
    "3. delta = it contains the declination angle, which shows the angle between the celestial equator and a point on the celestial sphere. It is like the latitude in space which results can either be positive or negative. Positive means that the object is located north while negative means it is located south.\n",
    "4. u – Ultraviolet filter in the photometric system.\n",
    "5. g – Green filter in the photometric system.\n",
    "6. r – Red filter in the photometric system.\n",
    "7. i – Near Infrared filter in the photometric system.\n",
    "8. z – Infrared filter in the photometric system.\n",
    "9. run_ID – Run Number used to identify the specific scan.\n",
    "10. rereun_ID – Rerun Number to specify how the image was processed.\n",
    "11. cam_col – Camera column to identify the scanline within the run.\n",
    "12. field_ID – Field number to identify each field.\n",
    "13. spec_obj_ID – Unique ID used for optical spectroscopic objects (this means that 2 different observations with the same spec_obj_ID must share the output class).\n",
    "14. class – object class (galaxy, star or quasar object). This is the label the group will be tasking the models to predict.\n",
    "15. redshift – redshift value based on the increase in wavelength.\n",
    "16. plate – plate ID, identifies each plate in SDSS (Sloan Digital Sky Survey).\n",
    "17. MJD – Modified Julian Date, used to indicate when a given piece of SDSS (Sloan Digital Sky Survey) data was taken.\n",
    "18. fiber_ID – identifies the fiber that pointed the light at the focal plane in each observation.\n",
    "\n",
    "The first few rows of the dataset look like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "seed_num = 12\n",
    "\n",
    "np.random.seed(seed_num)\n",
    "\n",
    "stars = pd.read_csv('stars.csv')\n",
    "print(\"stars dataset shape: \", stars.shape)\n",
    "stars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows the number of data points for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stars['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows the datatypes of each feature within the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stars.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of requirements\n",
    "\n",
    "The group will be using Python's scikit learn library for their classification models (I think), scipy, scikit, numpy and pandas array for processing data, and matplotlib for visualizing data (add stuff if need pa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Cleaning and Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we assign a set of columns that will be the focus for data cleaning and preprocessing, namely the `u`, `g`, `r`, `i`, `z`, and `redshift` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns_to_clean = ['alpha', 'delta', 'u', 'g', 'r', 'i', 'z', 'redshift']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacement of `obj_ID` column\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `obj_ID` column assigns a unique ID for each observation in the dataset, although it is represented in a floating point format. For the purposes of the study, this will be replaced with a similarly-named column that utilizes integer IDs instead in order to simplify the access to each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stars = stars.drop(['obj_ID'], axis=1)\n",
    "stars['obj_ID'] = np.arange(len(stars))\n",
    "stars.insert(0, 'obj_ID', stars.pop('obj_ID'))\n",
    "stars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for empty observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking was done for any entries with missing/null features by instantiating another stars dataset whose entries are filtered off of null features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nan_variables = stars[stars.isnull().any(axis=1)].index\n",
    "stars_nonull = stars.drop(labels=nan_variables).reset_index(drop=True)\n",
    "print(\"stars dataset shape (no null): \", stars_nonull.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown by the resulting filtered dataset and comparing its shape to the original dataset, it was shown that all entries have all their features accounted for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for duplicate entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any duplicated entries was checked first by comparing all entries' columns to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stars.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "As shown, there are no duplicated entries within the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The removal of outliers aims to craft a dataset whose subset can be used for training data that is generally representative of each classes. We will first check for the existence of outliers in this instance, and any suspicious outliers that implies default/placeholder values will be removed from analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stars[(np.abs(stats.zscore(stars[columns_to_clean])) < 3).all(axis=1)].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2116 data points that are considered as outliers based from the z-score metric (>= 3 standard deviations from the mean). In order to know more about these outliers, the features are drawn in a boxplot, displaying their minimum and maximum values respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in columns_to_clean:\n",
    "    stars.boxplot(col, figsize=(4, 4))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"\\\"\",col,\"\\\" minimum value: \", stars[col].min())\n",
    "    print(\"\\\"\",col,\"\\\" maximum value: \", stars[col].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features have values equated to a whole value of \\-9999. This may indicate that these features were not recorded/not available for that observation, thus putting a default value for those features. To resolve this, we will remove the associated entries that has default values for these certain features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_value = stars['u'].min()\n",
    "stars = stars[stars['u'] != default_value]\n",
    "stars = stars[stars['g'] != default_value]\n",
    "stars = stars[stars['z'] != default_value]\n",
    "stars.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based from the filtered dataset, there is only one entry that contains such default values. Any other outliers will be resolved by applying the robust scaling method for normalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in columns_to_clean:\n",
    "    stars.boxplot(col, figsize=(4, 4))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"\\\"\",col,\"\\\" minimum value: \", stars[col].min())\n",
    "    print(\"\\\"\",col,\"\\\" maximum value: \", stars[col].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplots indicates that any extreme outliers that comes from the default values were successfully removed. However, the `redshift` feature may require additional feature engineering in the form of logarithmic transformation, which will be done in the normalization phase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logarithmic transformation of the `redshift` feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `redshift` feature will be transformed by implementing a helper function to be used in the normalization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def log_transform_feature(df, feature):\n",
    "    feature_min = df[feature].min()\n",
    "    feature_max = df[feature].max()\n",
    "    # shifter to convert negaative values to positive, and to keep any values above zero for log computation\n",
    "    shifter = abs(feature_min) + 0.001\n",
    "        \n",
    "    df[feature] = np.log(df[feature] + shifter)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing `obj_ID` after the removal of outliers\n",
    "\n",
    "Since the outliers were removed, the row obj_ID may not reflect it's actual position in the dataset. To fix this, we simply reassign the obj_IDs incrementally.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stars = stars.drop(['obj_ID'], axis=1)\n",
    "stars['obj_ID'] = np.arange(len(stars))\n",
    "stars.insert(0, 'obj_ID', stars.pop('obj_ID'))\n",
    "stars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is performed for the previously\\-specified columns in order to equally weight\\-in each of the features for the model training and evaluation. The robust scaler will be applied as normalization technique to account for the massive outliers for some features such as `u`, `g,` and `z`.\n",
    "\n",
    "In order to begin the normalization process, the original dataset is split into training and test dataset alongside splitting their features and labels, and are then normalized. This was done to simulate the prediction on unseen data by only doing the normalization computations within the split datasets themselves to avoid imposing any bias for the evaluation. Additionally, the redshift feature is log\\-transformed first in order to extract more range from its skewed distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# For use in EDA\n",
    "og_stars = stars.copy()\n",
    "\n",
    "cols = ['u','g','r','i','z','redshift']\n",
    "X = stars[cols]\n",
    "y = stars[\"class\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=seed_num, test_size=0.3, stratify=y)\n",
    "\n",
    "# log-transform redshift features first\n",
    "X_train = log_transform_feature(X_train, \"redshift\")\n",
    "scaler = RobustScaler().fit(X_train[cols])\n",
    "X_train[cols] = scaler.transform(X_train[cols])\n",
    "X_train = X_train.to_numpy()\n",
    "\n",
    "X_test = log_transform_feature(X_test, \"redshift\")\n",
    "scaler = RobustScaler().fit(X_test[cols])\n",
    "X_test[cols] = scaler.transform(X_test[cols])\n",
    "X_test = X_test.to_numpy()\n",
    "\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original stars dataset is also normalized for comparison in the EDA process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "stars = log_transform_feature(stars, \"redshift\")\n",
    "scaler = RobustScaler().fit(stars[columns_to_clean])\n",
    "stars[columns_to_clean] = scaler.transform(stars[columns_to_clean])\n",
    "stars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns_to_use = ['obj_ID','alpha','delta', 'u', 'g', 'r', 'i', 'z', 'redshift', 'class']\n",
    "columns_data = ['alpha','delta','u','g','r','i','z','redshift']\n",
    "\n",
    "labels = {'GALAXY':0,'STAR':1,'QSO':2}\n",
    "\n",
    "stars_df = stars[columns_to_use]\n",
    "stars_df['class'] = stars_df['class'].map(labels)\n",
    "\n",
    "stars_df.set_index('obj_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "stars_df.boxplot(columns_data, by=\"class\", figsize=(20,18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Tendencies\n",
    "\n",
    "To gain a better understanding of the data, we can begin with examining the central tendencies of the data. These would be the mean, median and mode.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Means\n",
    "\n",
    "For the means, we can take the means of each feature, pertaining to each class and compare them to one another. Putting the means together in a table, it would look like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "means_df = og_stars.groupby('class').mean()\n",
    "means_df = means_df[['alpha','delta','u','g','r','i','z','redshift']].T\n",
    "\n",
    "normalized_means_df = stars.groupby('class').mean()\n",
    "normalized_means_df = normalized_means_df[['alpha','delta','u','g','r','i','z','redshift']].T\n",
    "\n",
    "print(means_df)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(normalized_means_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then graph it to a bar plot to make it easier to compare to one another. It would look like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(means_df), figsize=(8, 4 * len(means_df)))\n",
    "\n",
    "# Iterate through each row and plot a bar graph\n",
    "for i, (index, row) in enumerate(means_df.iterrows()):\n",
    "    # Create a bar plot for each row\n",
    "    ax = axes[i]\n",
    "    row.plot(kind='bar', ax=ax, title=f'Normalized Row {index}')\n",
    "    ax.set_ylabel('Values')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=len(normalized_means_df), figsize=(8, 4 * len(normalized_means_df)))\n",
    "\n",
    "# Iterate through each row and plot a bar graph\n",
    "for i, (index, row) in enumerate(normalized_means_df.iterrows()):\n",
    "    # Create a bar plot for each row\n",
    "    ax = axes[i]\n",
    "    row.plot(kind='bar', ax=ax, title=f'Normalized Row {index}')\n",
    "    ax.set_ylabel('Values')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis goes here, will add later\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mode_df = og_stars.groupby('class').apply(lambda x: x.mode().iloc[0])\n",
    "mode_df = mode_df[['alpha', 'delta', 'u','g','r','i','z','redshift']].T\n",
    "\n",
    "normalized_mode_df = stars.groupby('class').apply(lambda x: x.mode().iloc[0])\n",
    "normalized_mode_df = normalized_mode_df[['alpha','delta','u','g','r','i','z','redshift']].T\n",
    "\n",
    "print(mode_df)\n",
    "print(\"\\n\")\n",
    "print(normalized_mode_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=len(mode_df), figsize=(8, 4 * len(mode_df)))\n",
    "\n",
    "# Iterate through each row and plot a bar graph\n",
    "for i, (index, row) in enumerate(mode_df.iterrows()):\n",
    "    # Create a bar plot for each row\n",
    "    ax = axes[i]\n",
    "    row.plot(kind='bar', ax=ax, title=f'Row {index}')\n",
    "    ax.set_ylabel('Values')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=len(normalized_mode_df), figsize=(8, 4 * len(normalized_mode_df)))\n",
    "\n",
    "# Iterate through each row and plot a bar graph\n",
    "for i, (index, row) in enumerate(normalized_mode_df.iterrows()):\n",
    "    # Create a bar plot for each row\n",
    "    ax = axes[i]\n",
    "    row.plot(kind='bar', ax=ax, title=f'Normalized Row {index}')\n",
    "    ax.set_ylabel('Values')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "median_df = og_stars.groupby('class').median()\n",
    "median_df = median_df[['alpha','delta','u','g','r','i','z','redshift']].T\n",
    "\n",
    "normalized_median_df = stars.groupby('class').median()\n",
    "normalized_median_df = normalized_median_df[['alpha','delta','u','g','r','i','z','redshift']].T\n",
    "\n",
    "\n",
    "print(median_df)\n",
    "print(\"\\n\")\n",
    "print(normalized_median_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=len(median_df), figsize=(8, 4 * len(median_df)))\n",
    "\n",
    "# Iterate through each row and plot a bar graph\n",
    "for i, (index, row) in enumerate(median_df.iterrows()):\n",
    "    # Create a bar plot for each row\n",
    "    ax = axes[i]\n",
    "    row.plot(kind='bar', ax=ax, title=f'Row {index}')\n",
    "    ax.set_ylabel('Values')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=len(normalized_median_df), figsize=(8, 4 * len(normalized_median_df)))\n",
    "\n",
    "# Iterate through each row and plot a bar graph\n",
    "for i, (index, row) in enumerate(normalized_median_df.iterrows()):\n",
    "    # Create a bar plot for each row\n",
    "    ax = axes[i]\n",
    "    row.plot(kind='bar', ax=ax, title=f'Normalized Row {index}')\n",
    "    ax.set_ylabel('Values')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "stars_df.boxplot(columns_data, by=\"class\", figsize=(20,18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "The following are the models that will be utizlied for training, hyperparameter tuning and testing of model\n",
    "\n",
    "1. Logistic Regression\n",
    "2. Decision Trees\n",
    "3. K Nearest Neighbors \\(or\\)\n",
    "4. Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_train_logis = X_train.copy()\n",
    "y_train_logis = y_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "logistic_model = SGDClassifier(loss='log_loss', eta0=0.001, max_iter=200, learning_rate='constant', random_state=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "max_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from data_loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader(X_train_logis, y_train_logis, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "e = 0\n",
    "is_converged = False\n",
    "previous_loss = 0\n",
    "labels = np.unique(y_train_logis)\n",
    "\n",
    "# For each epoch\n",
    "while e < max_epochs and is_converged is not True:\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    X_batch, y_batch = data_loader.get_batch()\n",
    "    \n",
    "    for X, y in zip(X_batch, y_batch):\n",
    "        \n",
    "        logistic_model.partial_fit(X, y, classes=labels)\n",
    "        \n",
    "        y_pred_logis = logistic_model.predict_proba(X_train_logis)\n",
    "        loss += log_loss(y_train_logis, y_pred_logis)\n",
    "        \n",
    "    print('Epoch:', e + 1, '\\tLoss:', (loss / len(X_batch)))\n",
    "    \n",
    "    if abs(previous_loss - loss) < 0.05:\n",
    "        is_converged = True\n",
    "    else:\n",
    "        previous_loss = loss\n",
    "        e += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### ------END OF LOGISTIC REGRESSION------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors\n",
    "\n",
    "There is no training when utilizing K Nearest Neighbors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insight and conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
